<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>å®Œæ•´äººè‡‰è¨»å†Šèˆ‡è¾¨è­˜ç³»çµ±</title>
  <style>
    canvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    #video {
      position: relative;
    }
  </style>
</head>
<body>
  <h2>ðŸ‘¤ äººè‡‰è¨»å†Šèˆ‡è¾¨è­˜ç³»çµ±</h2>

  <video id="video" width="720" height="560" autoplay muted></video><br>
  <input type="text" id="nameInput" placeholder="è¼¸å…¥å§“åè¨»å†Š Webcam è‡‰éƒ¨æ¨£æœ¬">
  <button onclick="registerFace()">ðŸ“¸ Webcam è¨»å†Š</button><br><br>

  <h3>ðŸ“‚ ä¸Šå‚³åœ–ç‰‡è¨“ç·´æ¨£æœ¬</h3>
  <input type="file" accept="image/*" onchange="handleImageUpload(event)">
  <input type="text" id="uploadName" placeholder="è¼¸å…¥ç…§ç‰‡ä¸­äººå">
  <p id="uploadStatus">å°šæœªä¸Šå‚³åœ–ç‰‡</p>

  <h3>ðŸ’¾ åŒ¯å‡º / è¼‰å…¥ æ¨£æœ¬è³‡æ–™</h3>
  <button onclick="downloadData()">ðŸ“¥ åŒ¯å‡ºæ¨£æœ¬è³‡æ–™</button>
  <input type="file" accept=".json" onchange="loadData(event)">

  <p id="status">ç‹€æ…‹ï¼šç­‰å¾…è¨»å†Šæˆ–è¼‰å…¥è³‡æ–™</p>

  <script defer src="./dist/face-api.esm.js"></script>
  <script>
    const video = document.getElementById('video');
    const nameInput = document.getElementById('nameInput');
    const statusEl = document.getElementById('status');
    const uploadStatus = document.getElementById('uploadStatus');
    const descriptorsMap = {}; // åç¨±å°æ‡‰ descriptor é™£åˆ—

	var faceapi="";

    async function startVideo() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
      video.srcObject = stream;
    }

	async function loadModels() {
		const modelPath = './model/';
	  // load face-api models
	  // await faceapi.nets.tinyFaceDetector.load(modelPath); // using ssdMobilenetv1
	  await faceapi.nets.ssdMobilenetv1.load(modelPath);
	  await faceapi.nets.ageGenderNet.load(modelPath);
	  await faceapi.nets.faceLandmark68Net.load(modelPath);
	  await faceapi.nets.faceRecognitionNet.load(modelPath);
	  await faceapi.nets.faceExpressionNet.load(modelPath);
	  optionsSSDMobileNet = new faceapi.SsdMobilenetv1Options({ minConfidence: minScore, maxResults });
	  // check tf engine state
	  log(`Models loaded: ${str(faceapi.tf.engine().state.numTensors)} tensors`);
	}

    async function loadModels_old() {
      const MODEL_URL = './model/';
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
      await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);
    }

    async function registerFace() {
      const name = nameInput.value.trim();
      if (!name) return statusEl.innerText = 'â— è«‹è¼¸å…¥å§“å';

      const detection = await faceapi
        .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptor();

      if (!detection) return statusEl.innerText = 'âŒ æ²’åµæ¸¬åˆ°è‡‰';
      if (!descriptorsMap[name]) descriptorsMap[name] = [];
      descriptorsMap[name].push(detection.descriptor);
      statusEl.innerText = `âœ… ç‚º ${name} è¨»å†Š 1 ç­†æ¨£æœ¬`;
    }

    async function handleImageUpload(event) {
      const file = event.target.files[0];
      const name = document.getElementById('uploadName').value.trim();
      if (!file || !name) return uploadStatus.innerText = 'â— è«‹é¸åœ– + å§“å';

      const img = await loadImageFromFile(file);
      const detection = await faceapi
        .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptor();

      if (!detection) return uploadStatus.innerText = 'âŒ ç„¡è‡‰éƒ¨åµæ¸¬';
      if (!descriptorsMap[name]) descriptorsMap[name] = [];
      descriptorsMap[name].push(detection.descriptor);
      uploadStatus.innerText = `âœ… ç‚º ${name} åŠ å…¥ä¸€å¼µåœ–ç‰‡æ¨£æœ¬`;
    }

    function loadImageFromFile(file) {
      return new Promise((resolve, reject) => {
        const img = new Image();
        img.onload = () => resolve(img);
        img.onerror = reject;
        img.src = URL.createObjectURL(file);
      });
    }

    function downloadData() {
      const jsonData = {};
      for (const [name, descriptors] of Object.entries(descriptorsMap)) {
        jsonData[name] = descriptors.map(d => Array.from(d));
      }
      const blob = new Blob([JSON.stringify(jsonData)], { type: 'application/json' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = 'face_descriptors.json';
      a.click();
      URL.revokeObjectURL(url);
      statusEl.innerText = 'âœ… åŒ¯å‡ºå®Œæˆ';
    }

    function loadData(event) {
      const file = event.target.files[0];
      if (!file) return;
      const reader = new FileReader();
      reader.onload = () => {
        const jsonData = JSON.parse(reader.result);
        for (const [name, list] of Object.entries(jsonData)) {
          descriptorsMap[name] = list.map(arr => new Float32Array(arr));
        }
        statusEl.innerText = `âœ… è¼‰å…¥ ${Object.keys(descriptorsMap).length} äººæ¨£æœ¬è³‡æ–™`;
      };
      reader.readAsText(file);
    }

    video.addEventListener('play', () => {
      const canvas = faceapi.createCanvasFromMedia(video);
      document.body.append(canvas);
      const displaySize = { width: video.width, height: video.height };
      faceapi.matchDimensions(canvas, displaySize);

      setInterval(async () => {
        const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceDescriptors();

        const resized = faceapi.resizeResults(detections, displaySize);
        canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);

        faceapi.draw.drawDetections(canvas, resized);
        faceapi.draw.drawFaceLandmarks(canvas, resized);

        const labeledDescriptors = Object.entries(descriptorsMap).map(([name, descriptors]) =>
          new faceapi.LabeledFaceDescriptors(name, descriptors)
        );

        if (labeledDescriptors.length > 0) {
          const matcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
          resized.forEach(detection => {
            const best = matcher.findBestMatch(detection.descriptor);
            const box = detection.detection.box;
            const drawBox = new faceapi.draw.DrawBox(box, { label: best.toString() });
            drawBox.draw(canvas);
          });
        }
      }, 200);
    });

    loadModels().then(startVideo);
  </script>
</body>
</html>
